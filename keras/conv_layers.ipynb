{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Convolutional Layers\n",
    "\n",
    "\n",
    "Keras Convolutional layers 총정리! 사용해보지 않거나 헷갈리는 layers 및 arguments에 초점을 맞춰서 정리해보았다.\n",
    "\n",
    "\n",
    "* Reference\n",
    "   * Keras > Documentation > LAYERS > [Convolutional Layers](https://keras.io/layers/convolutional/)\n",
    "\n",
    "\n",
    "※ 아래 기술한 모든 input/output shape은 `data_format='channels_last'` 기준\n",
    "\n",
    "\n",
    "Arguments\n",
    "\n",
    "\n",
    "* `filters`: Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution).\n",
    "* `kernel_size`\n",
    "   * 1D: An integer or tuple/list of a single integer, specifying the length of the 1D convolution window.\n",
    "   * 2D: An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions.\n",
    "* `padding`: One of \"valid\", \"causal\" or \"same\" (case-insensitive).\n",
    "   * `valid`: **No padding**\n",
    "   * `same`: **Zero padding** (the output has the same length as the original input.)\n",
    "   * 'causal`: **Causal (dilated) convolutions**, e.g. output[t] does not depend on input[t + 1:].\n",
    "      * For 1D convolution only.\n",
    "      * Output은 zero padding을 통해 input과 동일한 spatial resolution을 유지한다.\n",
    "* `dilation_rate`\n",
    "   * 1D: An integer or tuple/list of a single integer, specifying the dilation rate to use for dilated convolution. \n",
    "   * 2D: An integer or tuple/list of 2 integers, specifying the dilation rate to use for dilated convolution.\n",
    "   * Currently, specifying any dilation_rate value != 1 is incompatible with specifying any strides value != 1.\n",
    "* `initializer`: 생략\n",
    "* `regularizer`: 생략\n",
    "* `constraint`: 생략"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'conv1d_1/add:0' shape=(?, 16, 4) dtype=float32>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = layers.Input(shape=(16, 3))\n",
    "conv = layers.Conv1D(4, 3, padding='causal')(inputs)\n",
    "\n",
    "conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution\n",
    "\n",
    "\n",
    "Output volume (2D 기준):\n",
    "\n",
    "\n",
    "$$\n",
    "{ W }_{ out }=\\frac { { W }_{ in }-F+2P }{ S } +1\\quad ,\\quad { H }_{ out }=\\frac { { H }_{ in }-F+2P }{ S } +1\\quad ,\\quad { D }_{ out }=K\n",
    "$$\n",
    "\n",
    "\n",
    "* $W$ : width (columns)\n",
    "* $H$ : height (rows)\n",
    "* $D$ : depth (channel)\n",
    "* $F$ : the size of kernels (filters)\n",
    "* $K$ : the number of kernels (filters)\n",
    "* $S$ : stride\n",
    "* $P$ : padding\n",
    "\n",
    "\n",
    "### Conv1D\n",
    "\n",
    "\n",
    "```python\n",
    "keras.layers.Conv1D(filters,\n",
    "                    kernel_size,\n",
    "                    strides=1, padding='valid', data_format='channels_last',\n",
    "                    dilation_rate=1, activation=None, use_bias=True,\n",
    "                    kernel_initializer='glorot_uniform', bias_initializer='zeros',\n",
    "                    kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None,\n",
    "                    kernel_constraint=None, bias_constraint=None)\n",
    "```\n",
    "\n",
    "\n",
    "1D convolution layer (e.g. temporal convolution).\n",
    "\n",
    "\n",
    "Input shape: `(batch, steps, channels)`\n",
    "\n",
    "\n",
    "Output shape: `(batch, new_steps, filters)`\n",
    "\n",
    "\n",
    "### Conv2D\n",
    "\n",
    "\n",
    "```python\n",
    "keras.layers.Conv2D(filters,\n",
    "                    kernel_size,\n",
    "                    strides=(1, 1), padding='valid', data_format=None,\n",
    "                    dilation_rate=(1, 1), activation=None, use_bias=True,\n",
    "                    kernel_initializer='glorot_uniform', bias_initializer='zeros',\n",
    "                    kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None,\n",
    "                    kernel_constraint=None, bias_constraint=None)\n",
    "```\n",
    "\n",
    "\n",
    "2D convolution layer (e.g. spatial convolution over images).\n",
    "\n",
    "\n",
    "Input shape: `(batch, rows, cols, channels)`\n",
    "\n",
    "\n",
    "Output shape: `(batch, new_rows, new_cols, filters)`\n",
    "\n",
    "\n",
    "### Conv3D\n",
    "\n",
    "\n",
    "```python\n",
    "keras.layers.Conv3D(filters,\n",
    "                    kernel_size,\n",
    "                    strides=(1, 1, 1), padding='valid', data_format=None,\n",
    "                    dilation_rate=(1, 1, 1), activation=None, use_bias=True,\n",
    "                    kernel_initializer='glorot_uniform', bias_initializer='zeros',\n",
    "                    kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None,\n",
    "                    kernel_constraint=None, bias_constraint=None)\n",
    "```\n",
    "\n",
    "\n",
    "3D convolution layer (e.g. spatial convolution over volumes).\n",
    "\n",
    "\n",
    "Input shape: `(batch, conv_dim1, conv_dim2, conv_dim3, channels)`\n",
    "\n",
    "\n",
    "Output shape: `(batch, new_conv_dim1, new_conv_dim2, new_conv_dim3, filters)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depthwise Separable Convolution\n",
    "\n",
    "\n",
    "Separable convolutions consist in first performing a **depthwise spatial convolution** (which acts on each input channel separately) followed by a **pointwise convolution** which mixes together the resulting output channels.\n",
    "\n",
    "\n",
    "Arguments\n",
    "\n",
    "\n",
    "* `depth_multiplier`: The number of depthwise convolution output channels for each input channel. The total number of depthwise convolution output channels will be equal to `filters_in * depth_multiplier`.\n",
    "\n",
    "\n",
    "### SeparableConv1D\n",
    "\n",
    "\n",
    "```python\n",
    "keras.layers.SeparableConv1D(filters,\n",
    "                             kernel_size,\n",
    "                             strides=1, padding='valid', data_format='channels_last',\n",
    "                             dilation_rate=1, depth_multiplier=1, activation=None, use_bias=True,\n",
    "                             depthwise_initializer='glorot_uniform', pointwise_initializer='glorot_uniform', bias_initializer='zeros',\n",
    "                             depthwise_regularizer=None, pointwise_regularizer=None, bias_regularizer=None, activity_regularizer=None,\n",
    "                             depthwise_constraint=None, pointwise_constraint=None, bias_constraint=None)\n",
    "```\n",
    "\n",
    "\n",
    "Depthwise separable 1D convolution with pointwise convolution.\n",
    "\n",
    "\n",
    "Input shape: `(batch, steps, channels)`\n",
    "\n",
    "\n",
    "Output shape: `(batch, new_steps, filters)`\n",
    "\n",
    "\n",
    "### SeparableConv2D\n",
    "\n",
    "\n",
    "```python\n",
    "keras.layers.SeparableConv2D(filters,\n",
    "                             kernel_size,\n",
    "                             strides=(1, 1), padding='valid', data_format=None,\n",
    "                             dilation_rate=(1, 1), depth_multiplier=1, activation=None, use_bias=True,\n",
    "                             depthwise_initializer='glorot_uniform', pointwise_initializer='glorot_uniform', bias_initializer='zeros',\n",
    "                             depthwise_regularizer=None, pointwise_regularizer=None, bias_regularizer=None, activity_regularizer=None,\n",
    "                             depthwise_constraint=None, pointwise_constraint=None, bias_constraint=None)\n",
    "```\n",
    "\n",
    "\n",
    "Depthwise separable 2D convolution with pointwise convolution.\n",
    "\n",
    "\n",
    "Input shape: `(batch, rows, cols, channels)`\n",
    "\n",
    "\n",
    "Output shape: `(batch, new_rows, new_cols, filters)`\n",
    "\n",
    "\n",
    "### DepthwiseConv2D\n",
    "\n",
    "\n",
    "```python\n",
    "keras.layers.DepthwiseConv2D(kernel_size,\n",
    "                             strides=(1, 1), padding='valid', data_format=None,\n",
    "                             depth_multiplier=1, activation=None, use_bias=True,\n",
    "                             depthwise_initializer='glorot_uniform', bias_initializer='zeros',\n",
    "                             depthwise_regularizer=None, bias_regularizer=None, activity_regularizer=None,\n",
    "                             depthwise_constraint=None, bias_constraint=None)\n",
    "```\n",
    "\n",
    "\n",
    "Depthwise separable 2D convolution.\n",
    "\n",
    "\n",
    "Depthwise separable convolutions consists in performing just the first step in a separable convolution (which acts on each input channel separately). \n",
    "\n",
    "\n",
    "Input shape: `(batch, rows, cols, channels)`\n",
    "\n",
    "\n",
    "Output shape: `(batch, new_rows, new_cols, channels)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'depthwise_conv2d_1/BiasAdd:0' shape=(?, 64, 64, 3) dtype=float32>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = layers.Input(shape=(64, 64, 3))\n",
    "conv = layers.DepthwiseConv2D(3, padding='same')(inputs)\n",
    "\n",
    "conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transposed Convolution\n",
    "\n",
    "\n",
    "The need for transposed convolutions generally arises from the desire to use a transformation going in **the opposite direction of a normal convolution**. Up sampling layer와 달리 weights가 존재한다. 즉, 학습이 가능하다.\n",
    "\n",
    "\n",
    "* Reference\n",
    "   * [Up-sampling with Transposed Convolution](https://towardsdatascience.com/up-sampling-with-transposed-convolution-9ae4f2df52d0)\n",
    "   * [Is the Transposed Convolution layer and Convolution layer the same thing? Experimenting with concepts using PyTorch.](https://towardsdatascience.com/is-the-transposed-convolution-layer-and-convolution-layer-the-same-thing-8655b751c3a1)\n",
    "\n",
    "\n",
    "Output volume (2D 기준):\n",
    "\n",
    "\n",
    "$$\n",
    "{ W }_{ out }=S({ W }_{ in }-1)+F-2P\\quad ,\\quad { H }_{ out }=S({ H }_{ in }-1)+F-2P\\quad ,\\quad { D }_{ out }=K\n",
    "$$\n",
    "\n",
    "\n",
    "* $W$ : width (columns)\n",
    "* $H$ : height (rows)\n",
    "* $D$ : depth (channel)\n",
    "* $F$ : the size of kernels (filters)\n",
    "* $K$ : the number of kernels (filters)\n",
    "* $S$ : stride\n",
    "* $P$ : padding\n",
    "\n",
    "\n",
    "Arguments\n",
    "\n",
    "\n",
    "* `output_padding`\n",
    "   * 2D: An integer or tuple/list of 2 integers, specifying the amount of padding along the height and width of the output tensor.\n",
    "   * 3D: An integer or tuple/list of 3 integers, specifying the amount of padding along the depth, height, and width.\n",
    "   * Can be a single integer to specify the same value for all spatial dimensions. The amount of output padding along a given dimension must be lower than the stride along that same dimension. If set to None (default), the output shape is inferred.\n",
    "\n",
    "\n",
    "### Conv2DTranspose\n",
    "\n",
    "\n",
    "```python\n",
    "keras.layers.Conv2DTranspose(filters,\n",
    "                             kernel_size,\n",
    "                             strides=(1, 1), padding='valid', output_padding=None, data_format=None,\n",
    "                             dilation_rate=(1, 1), activation=None, use_bias=True,\n",
    "                             kernel_initializer='glorot_uniform', bias_initializer='zeros',\n",
    "                             kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None,\n",
    "                             kernel_constraint=None, bias_constraint=None)\n",
    "```\n",
    "\n",
    "\n",
    "Transposed convolution layer (sometimes called Deconvolution).\n",
    "\n",
    "\n",
    "Input shape: `(batch, rows, cols, channels)`\n",
    "\n",
    "\n",
    "Output shape: `(batch, new_rows, new_cols, filters)`\n",
    "\n",
    "\n",
    "### Conv3DTranspose\n",
    "\n",
    "\n",
    "```python\n",
    "keras.layers.Conv3DTranspose(filters,\n",
    "                             kernel_size,\n",
    "                             strides=(1, 1, 1), padding='valid', output_padding=None, data_format=None,\n",
    "                             activation=None, use_bias=True,\n",
    "                             kernel_initializer='glorot_uniform', bias_initializer='zeros',\n",
    "                             kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None,\n",
    "                             kernel_constraint=None, bias_constraint=None)\n",
    "```\n",
    "\n",
    "\n",
    "Transposed convolution layer (sometimes called Deconvolution).\n",
    "\n",
    "\n",
    "Input shape: `(batch, depth, rows, cols, channels)`\n",
    "\n",
    "\n",
    "Output shape: `(batch, new_depth, new_rows, new_cols, filters)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cropping\n",
    "\n",
    "\n",
    "Arguments\n",
    "\n",
    "\n",
    "* `output_padding`\n",
    "   * 1D: int or tuple of int (length 2).\n",
    "   * 2D: int, or tuple of 2 ints, or tuple of 2 tuples of 2 ints.\n",
    "   * 3D: int, or tuple of 3 ints, or tuple of 3 tuples of 2 ints.\n",
    "   * How many units should be trimmed off at the beginning and end of the cropping dimension. If a single int is provided, the same value will be used for both.\n",
    "\n",
    "\n",
    "### Cropping1D\n",
    "\n",
    "\n",
    "```python\n",
    "keras.layers.Cropping1D(cropping=(1, 1))\n",
    "```\n",
    "\n",
    "\n",
    "Cropping layer for 1D input (e.g. temporal sequence).\n",
    "\n",
    "\n",
    "Input shape: `(batch, steps, channels)`\n",
    "\n",
    "\n",
    "Output shape: `(batch, cropped_steps, channels)`\n",
    "\n",
    "\n",
    "### Cropping2D\n",
    "\n",
    "\n",
    "```python\n",
    "keras.layers.Cropping2D(cropping=((0, 0), (0, 0)), data_format=None)\n",
    "```\n",
    "\n",
    "\n",
    "Cropping layer for 2D input (e.g. picture).\n",
    "\n",
    "\n",
    "Input shape: `(batch, rows, cols, channels)`\n",
    "\n",
    "\n",
    "Output shape: `(batch, cropped_rows, cropped_cols, channels)`\n",
    "\n",
    "\n",
    "### Cropping3D\n",
    "\n",
    "\n",
    "```python\n",
    "keras.layers.Cropping3D(cropping=((1, 1), (1, 1), (1, 1)), data_format=None)\n",
    "```\n",
    "\n",
    "\n",
    "Cropping layer for 3D data (e.g. spatial or spatio-temporal).\n",
    "\n",
    "\n",
    "Input shape: `(batch, first_axis_to_crop, second_axis_to_crop, third_axis_to_crop, channels)`\n",
    "\n",
    "\n",
    "Output shape: `(batch, first_cropped_axis, second_cropped_axis, third_cropped_axis, channels)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'cropping1d_1/strided_slice:0' shape=(?, 10, 3) dtype=float32>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = layers.Input(shape=(16, 3))\n",
    "crop = layers.Cropping1D(cropping=3)(inputs)\n",
    "\n",
    "crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'cropping1d_2/strided_slice:0' shape=(?, 58, 3) dtype=float32>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = layers.Input(shape=(64, 3))\n",
    "crop = layers.Cropping1D(cropping=(3, 3))(inputs)\n",
    "\n",
    "crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'cropping1d_3/strided_slice:0' shape=(?, 61, 3) dtype=float32>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = layers.Input(shape=(64, 3))\n",
    "crop = layers.Cropping1D(cropping=(1, 2))(inputs)\n",
    "\n",
    "crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'cropping2d_1/strided_slice:0' shape=(?, 62, 60, 3) dtype=float32>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = layers.Input(shape=(64, 64, 3))\n",
    "crop = layers.Cropping2D(cropping=(1, 2))(inputs)\n",
    "\n",
    "crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'cropping2d_2/strided_slice:0' shape=(?, 61, 57, 3) dtype=float32>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = layers.Input(shape=(64, 64, 3))\n",
    "crop = layers.Cropping2D(cropping=((1, 2), (3, 4)))(inputs)\n",
    "\n",
    "crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'cropping3d_1/strided_slice:0' shape=(?, 62, 60, 58, 3) dtype=float32>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = layers.Input(shape=(64, 64, 64, 3))\n",
    "crop = layers.Cropping3D(cropping=(1, 2, 3))(inputs)\n",
    "\n",
    "crop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Up Sampling\n",
    "\n",
    "\n",
    "Interpolation으로 up sampling한다.\n",
    "\n",
    "\n",
    "Arguments\n",
    "\n",
    "\n",
    "* `size`: Upsampling factors.\n",
    "   * 1D: integer.\n",
    "   * 2D: int, or tuple of 2 integers. The upsampling factors for rows and columns.\n",
    "   * 3D: int, or tuple of 3 integers. The upsampling factors for dim1, dim2 and dim3.\n",
    "\n",
    "\n",
    "### UpSampling1D\n",
    "\n",
    "\n",
    "```python\n",
    "keras.layers.UpSampling1D(size=2)\n",
    "```\n",
    "\n",
    "\n",
    "Upsampling layer for 1D inputs.\n",
    "\n",
    "\n",
    "Repeats each temporal step size times along the time axis.\n",
    "\n",
    "\n",
    "Input shape: `(batch, steps, channels)`\n",
    "\n",
    "\n",
    "Output shape: `(batch, upsampled_steps, channels)`\n",
    "\n",
    "\n",
    "### UpSampling2D\n",
    "\n",
    "\n",
    "```python\n",
    "keras.layers.UpSampling2D(size=(2, 2), data_format=None, interpolation='nearest')\n",
    "```\n",
    "\n",
    "\n",
    "Upsampling layer for 2D inputs.\n",
    "\n",
    "\n",
    "Repeats the rows and columns of the data by size[0] and size[1] respectively.\n",
    "\n",
    "\n",
    "Arguments\n",
    "\n",
    "\n",
    "* `interpolation`: A string, one of `'nearest'` or `'bilinear'`.\n",
    "\n",
    "\n",
    "Input shape: `(batch, rows, cols, channels)`\n",
    "\n",
    "\n",
    "Output shape: `(batch, upsampled_rows, upsampled_cols, channels)`\n",
    "\n",
    "\n",
    "### UpSampling3D\n",
    "\n",
    "\n",
    "```python\n",
    "keras.layers.UpSampling3D(size=(2, 2, 2), data_format=None)\n",
    "```\n",
    "\n",
    "\n",
    "Upsampling layer for 3D inputs.\n",
    "\n",
    "\n",
    "Input shape: `(batch, dim1, dim2, dim3, channels)`\n",
    "\n",
    "\n",
    "Output shape: `(batch, upsampled_dim1, upsampled_dim2, upsampled_dim3, channels)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'up_sampling1d_1/concat:0' shape=(?, 128, 3) dtype=float32>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = layers.Input(shape=(64, 3))\n",
    "crop = layers.UpSampling1D(size=2)(inputs)\n",
    "\n",
    "crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'up_sampling2d_1/ResizeNearestNeighbor:0' shape=(?, 128, 128, 3) dtype=float32>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = layers.Input(shape=(64, 64, 3))\n",
    "crop = layers.UpSampling2D(size=2)(inputs)\n",
    "\n",
    "crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'up_sampling2d_2/ResizeNearestNeighbor:0' shape=(?, 64, 128, 3) dtype=float32>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = layers.Input(shape=(64, 64, 3))\n",
    "crop = layers.UpSampling2D(size=(1, 2))(inputs)\n",
    "\n",
    "crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'up_sampling3d_1/concat_2:0' shape=(?, 64, 128, 192, 3) dtype=float32>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = layers.Input(shape=(64, 64, 64, 3))\n",
    "crop = layers.UpSampling3D(size=(1, 2, 3))(inputs)\n",
    "\n",
    "crop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding\n",
    "\n",
    "\n",
    "Arguments\n",
    "\n",
    "\n",
    "* `padding`\n",
    "   * 1D: int, or tuple of int (length 2), or dictionary.\n",
    "   * 2D: int, or tuple of 2 ints, or tuple of 2 tuples of 2 ints.\n",
    "   * 3D: int, or tuple of 3 ints, or tuple of 3 tuples of 2 ints.\n",
    "\n",
    "\n",
    "### ZeroPadding1D\n",
    "\n",
    "\n",
    "```python\n",
    "keras.layers.UpSampling1D(size=2)\n",
    "```\n",
    "\n",
    "\n",
    "Zero-padding layer for 1D input (e.g. temporal sequence).\n",
    "\n",
    "\n",
    "Input shape: `(batch, steps, channels)`\n",
    "\n",
    "\n",
    "Output shape: `(batch, padded_steps, channels)`\n",
    "\n",
    "\n",
    "### ZeroPadding2D\n",
    "\n",
    "\n",
    "```python\n",
    "keras.layers.ZeroPadding2D(padding=(1, 1), data_format=None)\n",
    "```\n",
    "\n",
    "\n",
    "Zero-padding layer for 2D input (e.g. picture).\n",
    "\n",
    "\n",
    "Input shape: `(batch, rows, cols, channels)`\n",
    "\n",
    "\n",
    "Output shape: `(batch, padded_rows, padded_cols, channels)`\n",
    "\n",
    "\n",
    "### ZeroPadding3D\n",
    "\n",
    "\n",
    "```python\n",
    "keras.layers.ZeroPadding3D(padding=(1, 1, 1), data_format=None)\n",
    "```\n",
    "\n",
    "\n",
    "Zero-padding layer for 3D data (spatial or spatio-temporal).\n",
    "\n",
    "\n",
    "Input shape: `(batch, first_axis_to_pad, second_axis_to_pad, third_axis_to_pad, channels)`\n",
    "\n",
    "\n",
    "Output shape: `(batch, first_padded_axis, second_padded_axis, third_axis_to_pad, channels)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'zero_padding1d_1/Pad:0' shape=(?, 68, 3) dtype=float32>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = layers.Input(shape=(64, 3))\n",
    "pad = layers.ZeroPadding1D(padding=2)(inputs)\n",
    "\n",
    "pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'zero_padding2d_1/Pad:0' shape=(?, 68, 68, 3) dtype=float32>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = layers.Input(shape=(64, 64, 3))\n",
    "pad = layers.ZeroPadding2D(padding=2)(inputs)\n",
    "\n",
    "pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'zero_padding2d_2/Pad:0' shape=(?, 66, 68, 3) dtype=float32>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = layers.Input(shape=(64, 64, 3))\n",
    "pad = layers.ZeroPadding2D(padding=(1, 2))(inputs)\n",
    "\n",
    "pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'zero_padding2d_3/Pad:0' shape=(?, 67, 71, 3) dtype=float32>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = layers.Input(shape=(64, 64, 3))\n",
    "pad = layers.ZeroPadding2D(padding=((1, 2), (3, 4)))(inputs)\n",
    "\n",
    "pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'zero_padding3d_1/Pad:0' shape=(?, 68, 68, 68, 3) dtype=float32>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = layers.Input(shape=(64, 64, 64, 3))\n",
    "pad = layers.ZeroPadding3D(padding=2)(inputs)\n",
    "\n",
    "pad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
